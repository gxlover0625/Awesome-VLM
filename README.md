<div align="center">

# Awesome Vision-Language Model

[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

</div>

## üî• News
- **2025.06**: This repository shares the latest papers, tools, and open-source projects in the VLM / MLLM field and **will be regularly updated**.

## üåü Introduction
I am a researcher specializing in VLM / MLLM technology at Zhejiang University. This curated repository has been established with the aim of sharing cutting-edge papers, innovative technologies, and notable open-source projects within this domain.

## üìù  Paper
### Multi-Image Benchmark
- [2506] **[Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](http://www.arxiv.org/pdf/2506.04280)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Ziming Cheng, Binrui Xu, Lisheng Gong, Zuhe Song, Tianshuo Zhou, Shiqi Zhong, Siyu Ren, Mingxiang Chen, Xiangchao Meng, Yuxin Zhang, Yanlin Li, Lei Ren, Wei Chen, Zhiyuan Huang, Mingjie Zhan, Xiaojie Wang, Fangxiang Feng*
  - [Code üíª] [MMRB](https://github.com/LesterGong/MMRB) ![Star](https://img.shields.io/github/stars/LesterGong/MMRB.svg?style=social&label=Star)
- [2406] **[MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](https://arxiv.org/pdf/2406.09411)** ![ICLR2025](https://img.shields.io/badge/ICLR-2025-blue)
  - *Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen*
  - [Code üíª] [MuirBench](https://github.com/muirbench/MuirBench) ![Star](https://img.shields.io/github/stars/muirbench/MuirBench.svg?style=social&label=Star)
- [2406] **[Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning](https://arxiv.org/pdf/2406.12742)** ![Arxiv](https://img.shields.io/badge/Arxiv-Paper-red)
  - *Bingchen Zhao, Yongshuo Zong, Letian Zhang, Timothy Hospedales*
  - [Dataset ü§ó] [MIRB](https://huggingface.co/datasets/VLLMs/MIRB)
- [2404] **[BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/pdf/2404.12390)** ![ECCV2024](https://img.shields.io/badge/ECCV-2024-blue)
  - *Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna*
  - [Code üíª] [BLINK_Benchmark](https://github.com/zeyofu/BLINK_Benchmark) ![Star](https://img.shields.io/github/stars/zeyofu/BLINK_Benchmark.svg?style=social&label=Star)


## üåã Foundation Model
## <img src="https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"  alt="qwen" style="width: 24px; height: 24px; border-radius: 50%; vertical-align: middle; margin-left: 8px;"> Qwen Series
- [Qwen2.5-VL](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5)
- [QVQ-72B-Preview](https://huggingface.co/Qwen/QVQ-72B-Preview)
- [Qwen2.5-Omni](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e)
- [Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)

## <img src="https://cdn-avatars.huggingface.co/v1/production/uploads/64006c09330a45b03605bba3/FvdxiTkTqH8rKDOzGZGUE.jpeg"  alt="opengvlab" style="width: 24px; height: 24px; border-radius: 50%; vertical-align: middle; margin-left: 8px;"> OpenGVLab Series
- [InternVL3](https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d)
- [InternVL2.5](https://huggingface.co/collections/OpenGVLab/internvl25-673e1019b66e2218f68d7c1c)
- [InternVL2.0](https://huggingface.co/collections/OpenGVLab/internvl20-667d3961ab5eb12c7ed1463e)

## <img src="https://cdn-avatars.huggingface.co/v1/production/uploads/1670387859384-633fe7784b362488336bbfad.png"  alt="openbmb" style="width: 24px; height: 24px; border-radius: 50%; vertical-align: middle; margin-left: 8px;"> OpenBMB Series
- [MiniCPM-o-2_6](https://huggingface.co/openbmb/MiniCPM-o-2_6)
- [MiniCPM-V-2_6](https://huggingface.co/openbmb/MiniCPM-V-2_6)

## <img src="https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"  alt="deepseek" style="width: 24px; height: 24px; border-radius: 50%; vertical-align: middle; margin-left: 8px;"> DeepSeek Series
- [DeepSeek-VL2](https://huggingface.co/collections/deepseek-ai/deepseek-vl2-675c22accc456d3beb4613ab)

## <img src="https://cdn-avatars.huggingface.co/v1/production/uploads/5f1158120c833276f61f1a84/HYIF0By10WazlTdVv3xp0.jpeg"  alt="llava" style="width: 24px; height: 24px; border-radius: 50%; vertical-align: middle; margin-left: 8px;"> LLaVa Series
- [LLaVA-Onevision](https://huggingface.co/collections/llava-hf/llava-onevision-66bb1e9ce8856e210a7ed1fe)
- [LLaVa-NeXT](https://huggingface.co/collections/llava-hf/llava-next-65f75c4afac77fd37dbbe6cf)

## <img src="https://cdn-avatars.huggingface.co/v1/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"  alt="moonshot" style="width: 24px; height: 24px; border-radius: 50%; vertical-align: middle; margin-left: 8px;"> MoonShot Series
- [Kimi-VL-A3B](https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85)

## üìö Tutorial
- [nanoVLM](https://github.com/huggingface/nanoVLM) The simplest, fastest repository for training/finetuning small-sized VLMs. ![Star](https://img.shields.io/github/stars/huggingface/nanoVLM.svg?style=social&label=Star)

## üî® Deploying
- [ollama](https://github.com/ollama/ollama) Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models. ![Star](https://img.shields.io/github/stars/ollama/ollama.svg?style=social&label=Star)
```bash
# Install ollama, For Linux System
curl -fsSL https://ollama.com/install.sh | sh

# Run ollama
ollama serve

# Pull and Run model
ollama run qwen2.5vl:latest
```
- [vllm](https://github.com/vllm-project/vllm) A high-throughput and memory-efficient inference and serving engine for LLMs ![Star](https://img.shields.io/github/stars/vllm-project/vllm.svg?style=social&label=Star)
- [lmdeploy](https://github.com/InternLM/lmdeploy) LMDeploy is a toolkit for compressing, deploying, and serving LLMs. ![Star](https://img.shields.io/github/stars/InternLM/lmdeploy.svg?style=social&label=Star)

## üê≥ Training
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024) ![Star](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory.svg?style=social&label=Star)
- [verl](https://github.com/volcengine/verl) Volcano Engine Reinforcement Learning for LLMs ![Star](https://img.shields.io/github/stars/volcengine/verl.svg?style=social&label=Star)
- [ms-swift](https://github.com/modelscope/ms-swift) Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025). ![Star](https://img.shields.io/github/stars/modelscope/ms-swift.svg?style=social&label=Star)
- [EasyR1](https://github.com/hiyouga/EasyR1) An Efficient, Scalable, Multi-Modality RL Training Framework based on veRL. ![Star](https://img.shields.io/github/stars/hiyouga/EasyR1.svg?style=social&label=Star)


## ‚ù§Ô∏è Other Awesome Projects
- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) Latest Advances on Multimodal Large Language Models. ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star)
- [Awesome-RL-based-Reasoning-MLLMs](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs) This repository provides valuable reference for researchers in the field of multimodality, please start your exploratory travel in RL-based Reasoning MLLMs! ![Star](https://img.shields.io/github/stars/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.svg?style=social&label=Star)